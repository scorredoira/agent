# ğŸ› ï¸ Principios de Desarrollo - V3 Agent  

## ğŸš« **Regla de Oro: NO HARDCODING**

**NUNCA implementes lÃ³gica hardcodeada sin consultar primero si hay una alternativa mejor.**

### âŒ **Evitar Siempre:**
- Keywords lists para detecciÃ³n de intenciones
- Patrones fijos de texto para anÃ¡lisis
- LÃ³gica if/else compleja para decisiones
- Mapeos estÃ¡ticos de consulta â†’ acciÃ³n
- Configuraciones fijas que podrÃ­an ser dinÃ¡micas

### âœ… **Enfoques Preferidos:**
1. **LLM-Based Decision Making**: Que el LLM analice y decida
2. **Function Calling**: Usar capacidades nativas de los LLM providers
3. **Dynamic Configuration**: ConfiguraciÃ³n flexible y extensible
4. **Plugin Architecture**: Sistemas extensibles por diseÃ±o
5. **Self-Describing Systems**: Que los componentes se describan a sÃ­ mismos

## ğŸ§  **Arquitectura de Herramientas**

### **Nuevo Enfoque: LLM-Driven Tool Selection**

```
Usuario consulta â†’ LLM analiza contexto â†’ LLM solicita herramientas â†’ Agente ejecuta â†’ LLM continÃºa
```

#### **ImplementaciÃ³n:**
1. **System Prompt DinÃ¡mico**: 
   - Lista automÃ¡tica de herramientas disponibles
   - Descripciones generadas desde las herramientas mismas
   - Contexto actualizado en tiempo real

2. **Function Calling Nativo**:
   - OpenAI: `tools` parameter con `function` objects
   - Anthropic: `tools` parameter con tool definitions  
   - Gemini: `tools` con `function_declarations`

3. **Tool Registry Auto-descriptivo**:
   - Cada herramienta se describe a sÃ­ misma
   - Schemas generados automÃ¡ticamente
   - Capabilities detectadas dinÃ¡micamente

### **Ejemplo de Flujo:**

```go
// âŒ MALO - Hardcoded
if strings.Contains(query, "authentication") {
    useKnowledgeBase = true
}

// âœ… BUENO - LLM decides
systemPrompt := buildDynamicSystemPrompt(toolRegistry)
llmResponse := llm.Complete(systemPrompt + userQuery)
if llmResponse.wantsToUseTool("kbase") {
    executeTool("kbase", llmResponse.toolParams)
}
```

## ğŸ“‹ **Proceso de DecisiÃ³n**

### **Antes de implementar, pregÃºntate:**

1. **Â¿Puede el LLM decidir esto mejor que cÃ³digo hardcodeado?** â†’ Probablemente SÃ
2. **Â¿Esta lÃ³gica serÃ¡ flexible para casos futuros?** â†’ Si no, replantearlo
3. **Â¿Estoy asumiendo patrones que podrÃ­an cambiar?** â†’ Hacerlo dinÃ¡mico
4. **Â¿Hay una forma mÃ¡s declarativa de expresar esto?** â†’ Usarla

### **Cuando Consultar:**
- **Siempre** antes de escribir listas de keywords
- **Siempre** antes de lÃ³gica if/else compleja para decisiones
- **Siempre** antes de mappings estÃ¡ticos
- Cuando sientes que estÃ¡s "adivinando" el comportamiento del usuario

## ğŸ¯ **Objetivos de DiseÃ±o**

### **Flexibilidad**
- El sistema debe adaptarse a nuevos casos sin cambios de cÃ³digo
- Las herramientas deben ser plug-and-play
- Las decisiones deben basarse en contexto, no reglas fijas

### **Auto-descripciÃ³n**
- Componentes que se explican a sÃ­ mismos
- Schemas generados automÃ¡ticamente
- DocumentaciÃ³n que emerge del cÃ³digo

### **LLM-Native**
- Aprovechar las capacidades de razonamiento del LLM
- Function calling como primitiva bÃ¡sica
- Context-aware decision making

## ğŸš€ **ImplementaciÃ³n Inmediata**

### **Tarea Actual: Tool Selection Refactor**

1. **Eliminar TaskPlanner keyword-based**
2. **Implementar Function Calling** para OpenAI/Anthropic/Gemini
3. **System Prompt DinÃ¡mico** con herramientas disponibles
4. **Auto-registration** de herramientas como functions
5. **LLM-driven execution** flow

### **CÃ³digo de Ejemplo:**

```go
// Tool auto-describes itself for LLM
type Tool interface {
    GetFunctionDefinition() *FunctionDefinition
    GetDescription() string
    GetParameterSchema() *JSONSchema
}

// System prompt builds itself
func (a *Agent) buildSystemPrompt() string {
    tools := a.registry.GetAvailableTools()
    return fmt.Sprintf(`
You are V3 Agent with access to these tools:
%s

Use tools when needed to provide accurate responses.
`, tools.ToFunctionDefinitions())
}
```

## ğŸ’¡ **Recordatorio**

**"If you find yourself writing hardcoded logic, stop and ask: Can the LLM do this better?"**

La respuesta casi siempre es SÃ. ğŸ§ âœ¨